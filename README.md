
üêæ KIRI ‚Äî A Tiny Curious Companion Powered by KIRI Core

A small embodied AI creature running on Raspberry Pi 5 with an IMX500 neural camera, expressive motion, and a very small attention span.

üåü Overview

KIRI is a micro-robotic creature built to feel curious, perceptive, and alive.
It lives on:

Raspberry Pi 5

IMX500 neural imaging sensor

Pan‚Äìtilt head with Arduino servo driver

Local TTS (Piper)

CPU face detection (YuNet)

Behaviour-driven motion

LLM conversational brain (optional)

KIRI is not humanoid.
It is not a cartoon.
It is more like a small house spirit that is mildly surprised to discover you exist.

üß† KIRI Core ‚Äî Architecture

KIRI Core is the modular framework behind:

Perception

Motion

Behaviour

Audio

State management

Integrations (LLM, IMX500 detections, etc.)

The architecture is intentionally simple, asynchronous, and fault-tolerant.

üó∫Ô∏è High-Level Architecture Diagram
                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                   ‚îÇ                KIRI CORE                ‚îÇ
                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                  ‚ñ≤               ‚ñ≤
                 Perception       ‚îÇ               ‚îÇ     Behaviour
                                  ‚îÇ               ‚îÇ
                                  ‚îÇ               ‚îÇ

         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ   Picamera2 (RGB)    ‚îÇ ‚îÇ   ‚îÇ        Behaviour Loops        ‚îÇ
         ‚îÇ  640√ó480 stable feed ‚îÇ ‚îÇ   ‚îÇ  - TrackFace                  ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ   ‚îÇ  - WakeUp / Sleep             ‚îÇ
                 ‚îÇ                ‚îÇ   ‚îÇ  - CuriousScan                ‚îÇ
     CPU Face Detection (YuNet)   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ                ‚îÇ                ‚îÇ
                 ‚ñº                ‚îÇ                ‚ñº

       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ  FACE REFINER     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ     SwivelMotion        ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ (smooth servo control)  ‚îÇ
                 ‚îÇ                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚ñº                                ‚îÇ
        Shared State (frame + faces)              ‚ñº
                 ‚îÇ                      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                 ‚îÇ                      ‚îÇ  SwivelController    ‚îÇ
                 ‚îÇ                      ‚îÇ (Arduino servo board)‚îÇ
                 ‚îÇ                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚ñº
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ    IMX500 Detector     ‚îÇ
      ‚îÇ (neural metadata only) ‚îÇ
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
      Object detection, cues,
     ambient awareness (future)

üì∏ Camera Architecture ‚Äî Why It Works This Way

This part is crucial for stability and future-proofing.

‚úî 1. Picamera2 provides ALL imaging

YuNet (face detector) needs:

Correct RGB

Predictable resolution

No overlays

No aspect-ratio trickery

Using IMX500 frames for CPU vision results in:

lag

bounding-box drift

servo ‚Äútornado mode‚Äù

IMX firmware upload stalls

general emotional distress

Therefore:
All OpenCV-based detection uses Picamera2 at 640√ó480 RGB.

This gives perfectly stable geometry ‚Üí perfectly stable tracking.

‚úî 2. IMX500 is used for neural inference ONLY

The IMX500 is not a camera.
It is a camera-shaped neural chip.

We use it only for:

onboard object detection

low-CPU awareness

future cues (person, pet, object, light, ‚Äúpresence‚Äù)

We never use its RGB output.

This matches Sony‚Äôs reference design and avoids:

DMA contention

allocator crashes

double-stream conflicts

Raspberry Pi kernel panics (the fun kind)

‚úî 3. Behaviour and motion depend on stable perception

Servo behaviour only works when fed:

stable bounding boxes

consistent timing

clean frames

By separating IMX500 metadata from Picamera2 imaging:

tracking stops oscillating

gaze stabilises

KIRI behaves like a creature instead of an industrial fan

‚úî 4. This design is modular and future-safe

Because the camera and detection systems are disentangled:

You can add gesture recognition

or IMX-based ‚Äúcuriosity triggers‚Äù

or person recognition

or ambient detection

or LLM multimodal reasoning

without rewriting the core.

üß± Project Structure
kiri-core/
‚îÇ
‚îú‚îÄ‚îÄ hardware/
‚îÇ   ‚îú‚îÄ‚îÄ swivel.py             # Arduino servo interface
‚îÇ   ‚îú‚îÄ‚îÄ imx500_detector.py    # Neural inference module
‚îÇ
‚îú‚îÄ‚îÄ motion/
‚îÇ   ‚îî‚îÄ‚îÄ swivel_motion.py      # Smooth, async servo control
‚îÇ
‚îú‚îÄ‚îÄ perception/
‚îÇ   ‚îú‚îÄ‚îÄ face_refiner.py       # YuNet face detector
‚îÇ   ‚îú‚îÄ‚îÄ face_provider.py      # Best-face selection
‚îÇ   ‚îî‚îÄ‚îÄ preview.py            # Local or web visualisation
‚îÇ
‚îú‚îÄ‚îÄ behaviour/
‚îÇ   ‚îú‚îÄ‚îÄ track_face.py         # Gaze tracking behaviour
‚îÇ   ‚îî‚îÄ‚îÄ wakeup.py             # Waking ritual
‚îÇ
‚îú‚îÄ‚îÄ runtime/
‚îÇ   ‚îú‚îÄ‚îÄ event_bus.py          # Publish/subscribe system
‚îÇ   ‚îú‚îÄ‚îÄ audio_manager.py      # Piper TTS
‚îÇ   ‚îú‚îÄ‚îÄ web_preview.py        # JPEG streaming server
‚îÇ
‚îî‚îÄ‚îÄ config/
    ‚îî‚îÄ‚îÄ models.py             # Paths to YuNet/embedders/etc.

üèÉ‚Äç‚ôÇÔ∏è Running the System
Minimal test:
python labs/face_detect_test.py

Full creature mode:
python labs/test_face_tracker.py


Once running:

See live preview at
http://raspberrypi.local:8080

KIRI will track your face

Servo motion is smooth

IMX500 draws detection boxes

YuNet feeds the behaviour system

üîÆ Future Additions (fully supported by this architecture)

IMX500-based ambient curiosity detection

‚ÄúLook-at-sound‚Äù microphone localisation

Emotional micro-movements (breathing, twitching)

LLM-based attention redirection

Person recognition + friendly greetings

Object-of-interest tracking

Scene curiosity scoring

KIRI is tiny, but the roadmap is not.

üìú Why This Architecture Wins

Stable

Predictable

Expandable

Uses IMX500 as intended

Keeps high-frequency control loops responsive

Avoids frame format hell

Fully asynchronous

Behaviour-driven (creature-like)

Most importantly:

It prevents KIRI from spinning wildly while claiming confidently that your face is ‚Äúsomewhere behind the radiator.‚Äù





# ‚≠ê KIRI ‚Äî A Tiny Curious Companion Powered by KIRI Core

A small, perceptive robotic creature with curiosity, presence-awareness, and expressive motion.

KIRI, think of a tiny kinetic creature that wakes up, looks around, recognizes familiar faces, greets returning humans, reacts with subtle gestures, and can chat using an LLM.
Designed to feel alive ‚Äî not humanoid, not cartoonish ‚Äî but like a small intelligent animal or house spirit.

**KIRI is powered by KIRI Core**, a modular framework for interactive robotic intelligence.

## üå± What is KIRI?

KIRI is a perceptive, expressive, behavior-driven micro-robot running on a Raspberry Pi with:

- a pan‚Äìtilt head
- a camera (IMX500 / PiCam2)
- local TTS (Piper)
- face recognition (yunet)
- presence tracking
- expressive gestures
- animated wake/sleep rituals
- LLM-based conversation

KIRI (is intended to) behave like a small curious creature ‚Äî a cross between:

- a pika
- an otter
- a ferret
- and a Nordic house spirit

It watches, recognizes, greets, and responds.

## üß¨ Personality & Design Philosophy

KIRI should feel:

- small
- curious
- attentive
- cuddly but not childish
- a bit mischievous
- alive

It doesn't aim to be humanoid.
Instead, KIRI behaves like a tiny, perceptive creature that shares your space.

## üß† What is KIRI Core?

Kernel for Interactive Robotic Intelligence**

It is the underlying framework that powers KIRI's:

- perception
- motion
- routines
- behaviors
- state transitions
- context awareness
- audio output
- integrations (LLM, OCR, etc.)

KIRI Core is modular, scalable, and designed to support additional sensors, behaviours, and "creature-like" interactions.

## üß± Folder Structure

```
kiri/
‚îÇ
‚îú‚îÄ‚îÄ core/                     # brain
‚îÇ   ‚îú‚îÄ‚îÄ event_bus.py          # pub/sub message system
‚îÇ   ‚îú‚îÄ‚îÄ state_machine.py      # Idle / Searching / Tracking / Talking / Sleeping
‚îÇ   ‚îú‚îÄ‚îÄ behaviour_engine.py   # chooses what KIRI does next
‚îÇ   ‚îú‚îÄ‚îÄ presence.py           # tracks who/when/return logic
‚îÇ   ‚îî‚îÄ‚îÄ config.py
‚îÇ
‚îú‚îÄ‚îÄ modules/                  # body + senses
‚îÇ   ‚îú‚îÄ‚îÄ swivel.py             # servo controller
‚îÇ   ‚îú‚îÄ‚îÄ motions.py            # small movement primitives
‚îÇ   ‚îú‚îÄ‚îÄ routines.py           # high-level sequences (wake-up, greet, sleep)
‚îÇ   ‚îú‚îÄ‚îÄ tts.py                # Piper TTS with auto USB/Bluetooth routing
‚îÇ   ‚îú‚îÄ‚îÄ camera_stream.py      # IMX500/PiCam capture
‚îÇ   ‚îú‚îÄ‚îÄ face_detect.py        # detection + refinement
‚îÇ   ‚îú‚îÄ‚îÄ face_id.py            # embeddings + DB
‚îÇ   ‚îú‚îÄ‚îÄ ocr.py                # optional OCR
‚îÇ   ‚îî‚îÄ‚îÄ llm_agent.py          # chat logic
‚îÇ
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ main.py               # starts everything
‚îÇ   ‚îî‚îÄ‚îÄ supervisor.py         # robustness & graceful shutdown
‚îÇ
‚îú‚îÄ‚îÄ scripts/                  # standalone utilities or tests
‚îÇ   ‚îî‚îÄ‚îÄ test_wakeup.py
‚îÇ
‚îú‚îÄ‚îÄ assets/
‚îÇ   ‚îú‚îÄ‚îÄ face_db/              # embeddings & stored identities
‚îÇ   ‚îî‚îÄ‚îÄ tts_cache/
‚îÇ
‚îî‚îÄ‚îÄ pyproject.toml            # makes KIRI installable (pip install -e .)
```



## üîç Presence & Greeting Logic

KIRI remembers:

- who it saw
- for how long
- when they left
- when they return

If you come back after a while, it will greet you warmly.
If you leave briefly, it won't spam greetings like an over-enthusiastic puppy.

## üß† Behavior Engine

The Behaviour Engine manages KIRI's brain state:

### States:

- **Idle** (resting, tiny micro-motions)
- **Searching** (scanning for humans)
- **Tracking** (following a detected face)
- **Talking** (LLM dialog mode)
- **Sleeping** (rest gesture, shutdown)

### Events:

Handled via the Event Bus:

- `FACE_DETECTED`
- `FACE_LOST`
- `PERSON_IDENTIFIED`
- `PERSON_RETURNED`
- `SYSTEM_WAKE`
- `SYSTEM_SLEEP`
- `GREET`
- `CHAT_REQUEST`

## üé§ Audio System

KIRI uses Piper TTS, with prio-based output routing:

1. USB speaker on `hw:2,0`
2. Bluetooth A2DP sink
3. Default PulseAudio/pipewire sink

This should guarantee that KIRI always has a voice.

## üêæ Motion Philosophy

KIRI's movements use the small-creature model:

- quick micro-nods
- curious glances
- slow scanning arcs
- slight jitter / micro-adjustments
- center‚Üítilt‚Üíreturn patterns

Movements should feel alive, non-robotic, and gently expressive.

## ü§ñ Chatting With KIRI

The module `llm_agent.py` handles:

- LLM interactions
- contextual conversation
- memory of recent dialog
- voice output pipeline

Using streaming models from OpenAI, Groq, or local models.



## üîÆ Roadmap

### v0.1 ‚Äî Foundations

- KIRI naming & identity
- Package structure
- Swivel & TTS fully integrated
- Wake-up routine
- Behaviour engine skeleton

### v0.2 ‚Äî Perception & Greetings

- Presence tracking
- Greeting logic
- Return recognition
- Search routine
- Idle motions

### v0.3 ‚Äî Conversational KIRI

- LLM chat integration
- Listening pose
- Talking pose
- Turn-taking logic

### v0.4 ‚Äî World Awareness

- OCR
- Object detection
- Context linking
- Memory system

### v0.5 ‚Äî Emotional Layer (Optional)

- mood based on interactions
- expressive motion variants
- personalization

## üßô Author Notes

KIRI is meant to be the smallest possible expression of:

> "A creature that lives with you, not a machine that runs next to you."

It's a robotics experiment in:

- perception
- expression
- personality
- state machines
- human‚ÄìAI interaction
- tiny magical behaviours

---

## üõ†Ô∏è Installing KIRI Core

KIRI requires a Raspberry Pi (4 or 5 recommended), Python 3.10+, and a pan‚Äìtilt servo controller.

This guide assumes:

- your Pi is up-to-date
- you are using a virtual environment with `--system-site-packages`
- you want KIRI to run with hardware acceleration, Piper TTS, and YuNet face detection

### 1Ô∏è‚É£ System Prerequisites

Update Raspberry Pi:

```bash
sudo apt update
sudo apt upgrade -y
sudo reboot
```

Install system packages:

```bash
sudo apt install -y \
    python3-venv python3-pip python3-dev \
    libatlas-base-dev libopenblas-dev \
    libjpeg-dev libpng-dev \
    libavcodec58 libavformat58 libswscale5 \
    portaudio19-dev ffmpeg \
    git curl
```

### 2Ô∏è‚É£ Create Virtual Environment

We strongly recommend allowing system packages (for speed and picamera):
DO NOT INSTALL pip picamera!!

```bash
python3 -m venv .venv --system-site-packages
source .venv/bin/activate
```

Upgrade tooling:

```bash
pip install --upgrade pip wheel setuptools
```

### 3Ô∏è‚É£ Install OpenCV (headless + contrib)

First remove any leftover OpenCV packages:

```bash
pip uninstall -y opencv-python opencv-python-headless opencv-contrib-python opencv-contrib-python-headless || true
```

Install the binary-only contrib build:

```bash
pip install --only-binary=:all: opencv-contrib-python-headless==4.10.0.84
```

Verify:

```bash
python3 - << 'EOF'
import cv2, sys
print("Python:", sys.version.split()[0])
print("OpenCV:", cv2.__version__)
print("FaceDetectorYN:", hasattr(cv2, "FaceDetectorYN"))
print("dnn module:", hasattr(cv2, "dnn"))
EOF
```

You must see:
`FaceDetectorYN: True` and `dnn module: True`.

### 4Ô∏è‚É£ Install KIRI Dependencies

Core libs:

```bash
pip install numpy pyserial onnxruntime
```

Speech:

```bash
pip install piper-tts
```

Optional for chat:

```bash
pip install openai groq
```

### 5Ô∏è‚É£ Download YuNet Face Detector

KIRI uses YuNet for lightweight face detection.

Download:

```bash
mkdir -p assets/models
curl -L \
  -o assets/models/face_detection_yunet_2023mar.onnx \
  https://huggingface.co/opencv/face_detection_yunet/resolve/main/face_detection_yunet_2023mar.onnx
```

### 6Ô∏è‚É£ Install KIRI

In the root of the repo:

```bash
pip install -e .
```

This makes the entire kiri package importable everywhere:

```python
from kiri.modules.tts import TTS
from kiri.modules.swivel import SwivelController
```

### 7Ô∏è‚É£ Verify Hardware

**‚úî Camera**

```bash
libcamera-still -o test.jpg
```

**‚úî Servo Controller**

```python
python3 - << 'EOF'
from kiri.modules.swivel import SwivelController
sw = SwivelController().open()
sw.center()
EOF
```

The head should move to 90¬∞/90¬∞.

### 8Ô∏è‚É£ Configure Bluetooth Speaker (Optional)

List sinks:

```bash
pactl list short sinks
```

Set default:

```bash
pactl set-default-sink bluez_output.YOUR_DEVICE
```

Verify:

```bash
pactl info | grep "Default Sink"
```

### 9Ô∏è‚É£ Test Piper TTS

```python
python3 - << 'EOF'
from kiri.modules.tts import TTS
tts = TTS()
tts.say("KIRI is ready.")
EOF
```

If it speaks ‚Äî everything works.

### üîü Test KIRI Wake-Up Routine

```bash
python3 scripts/test_wakeup.py
```

You should see:

- center
- twitch
- nod
- scan
- greet

## üéâ Installation Complete

Now KIRI is alive.
You officially have a tiny mythological house spirit living in your Raspberry Pi.

